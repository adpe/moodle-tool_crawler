


Task timing setup
----------------

Because this is continually hitting your prod system, you may want to only run the robot crawler at defined times, such as every minute but only during night hours.


Task API setup




 - crawler status
   - how many urls have been crawled this time?
   - how many new urls found
   - how many urls are not yet crawled
   - make this a page that refreshes every couple seconds



* the robot
 - set the site level scrape time to now
 - start at the seed
 - scrape a page
 - find links, create if needed the next pages, create if needed the links, touch the timestamp

 - and which isn't 'ignored' do we care?

* finishing a crawl

* when do we start a new recrawl? right away?


* orphans pages?
 - if broken, clean them up (ie url was a typo, so we get an entry that will never work, remove it later when it's edge is gone)


* orphaned links?
 - do we remove them?


* capability
 - ignore a broken page
 - ignore a broken course link


* report: admin and course level reports
 - internal urls which re busted, what pages are they on / course
 - cross course links (mostly help? maybe the umbrella courses)
 - most popular pages linked to

* do we also need a block to show some quick stats?
 - broken links, grouped by course pages


