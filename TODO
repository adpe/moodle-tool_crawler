

* make a settings page
 - auth method (one driver per auth pluin, only do basic)
 - what is the crawl user / password
 - what is the seed url
 - exclusion urls
 - query params to strip
 - last full scrape starting at seed url

 - auth tester

 - crawler settings
 - max crawls per cron tick
 - robot user agent string

 - crawler status
   - how many urls have been crawled this time?
   - how many new urls found
   - how many urls are not yet crawled
   - make this a page that refreshes every couple seconds

 - ignore links in contexts, ie 'block' ignore all links in blocks,
   or 'block_admin_bookmark' ignore links in the admin block
   essentially a bunch of css'ish selectors to cull from the dom


* make a node db table
 - url (with stripped params) primary
 - last scraped
 - first scraped?
 - touch timestamp - if this is newer than the site level 'lastscrape' then this url is due for rescraping
 - response code / header - if not yet scraped should be null
 - last response code?
 - course context id   fkey
 - module type id      fkey
 - activity context id fkey
 - mime type of the resource
 - ignored by userid
 - ignore by timestamp
 - failedtries
 - bytes of resource
 - redirect location

* make an edge table
 - from id
 - to id
 - course id
 - 

 - last seen timestamp
 - link context, is the link in a block? if so what is the block id
 - is the link in a book page, or forum post etc?
 - ignored by userid
 - ignore by timestamp

* the robot
 - set the site level scrape time to now
 - start at the seed
 - scrape a page
 - find links, create if needed the next pages, create if needed the links, touch the timestamp

* create a page
 - strip the url of bad query params like sess key
 - check it against the excluded urls
 - if it is a local url thats cool
 - don't create links from external to external urls


 - now look through the queue and grab the next 1 (or batch of X) and repeat
     find the oldest touched url,
     which is newer than site scrape,
     and which hasn't been scraped since the touch
     and which isn't 'ignored'


* orphans pages?
 - if broken, clean them up (ie url was a typo, so we get an entry that will never work, remove it later when it's edge is gone)


* orphaned links?
 - do we remove them?


* capability
 - ignore a broken page
 - ignore a broken course link


* report: admin and course level reports
 - internal urls which re busted, what pages are they on / course
 - cross course links (mostly help? maybe the umbrella courses)
 - most popular pages linked to

* do we also need a block to show some quick stats?
 - broken links, grouped by course pages


